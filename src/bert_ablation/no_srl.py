'''
no_srlï¼šä¸»è¦æ›´æ”¹ä½ç½® class TextProcessorï¼ˆç§»é™¤è¯­ä¹‰è§’è‰²æ ‡æ³¨SRLï¼‰
'''

import torch
from transformers import AutoTokenizer, AutoModel
from transformers import BertModel, BertTokenizer
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import json
from tqdm import tqdm
import re
from difflib import SequenceMatcher
import jieba
from hanlp_restful import HanLPClient
from keyword_config import TARGET_KEYWORDS, HATE_KEYWORDS
HanLP = HanLPClient('https://www.hanlp.com/api', auth='your_auth')  # authéœ€è¦ç”³è¯·

# è‡ªå®šä¹‰æ•°æ®é›†ç±» - ä¿®æ”¹ä¸ºæ”¯æŒå››å…ƒç»„ç”Ÿæˆ
class HateSpeechDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_length=128):
        with open(data_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.target_groups = [
            'Racism', 'non-hate', 'others', 'Region', 'LGBTQ', 'Sexism',
            'Sexism,Racism', 'Sexism, others', 'Sexism, Racism,others', 'Racism,others',
            'Region,Sexism', 'Region, Racism', 'Region, others', 'LGBTQ, Region',
            'others, Sexism', 'Region, LGBTQ', 'Sexism, Region', 'Racism,Sexism',
            'LGBTQ, others', 'Sexism,Racism,Region', 'LGBTQ,Region, others', 'LGBTQ, Sexism',
            'LGBTQ, Sexism, others', 'Region,Sexism,Racism', 'Racism,Region,Sexism',
            'Sexism, LGBTQ', 'LGBTQ,Sexism,Racism', 'LGBTQ,Racism', 'Region,Racism,others',
            'LGBTQ,Racism,others'
        ]
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        text = item['content']
        quads = self._parse_quads(item['output'], text)
        
        # å…ˆä½¿ç”¨tokenizerå¤„ç†æ–‡æœ¬
        inputs = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # å¤„ç†æ‰€æœ‰å››å…ƒç»„
        groups = []
        is_hates = []
        for quad in quads:
            # éªŒè¯groupæ˜¯å¦åœ¨target_groupsä¸­
            group = quad['group'] if quad['group'] in self.target_groups else 'others'
            groups.append(self.target_groups.index(group))
            is_hates.append(quad['is_hate'])
        
        # å¦‚æœæ²¡æœ‰å››å…ƒç»„ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not quads:
            groups = [self.target_groups.index('non-hate')]
            is_hates = [0]
        
        return {
            'input_ids': inputs['input_ids'].squeeze(),
            'attention_mask': inputs['attention_mask'].squeeze(),
            'groups': torch.tensor(groups),  # æ”¹ä¸ºå¤æ•°å½¢å¼
            'is_hates': torch.tensor(is_hates, dtype=torch.float)  # æ”¹ä¸ºå¤æ•°å½¢å¼
        }
    
    def _parse_quads(self, output, text):  # æ·»åŠ textå‚æ•°
        quad_list = []
        for quad in output.split(' [SEP] '):
            parts = quad.split(' | ')
            if len(parts) >= 4:
                target = parts[0]
                argument = parts[1]
                group = parts[2].split(',')[0].strip()
                
                # ç¬¬ä¸€å±‚åˆ¤æ–­ï¼šä¿ç•™åŸæœ‰çš„æ¨¡ç³ŠåŒ¹é…é€»è¾‘
                is_hate = False
                for kw in HATE_KEYWORDS:
                    ratio = SequenceMatcher(None, kw, argument).ratio()
                    if ratio >= 0.3:
                        is_hate = True
                        break
                
                # ç¬¬äºŒå±‚åˆ¤æ–­ï¼šå¦‚æœç¬¬ä¸€å±‚æœªæ£€æµ‹åˆ°ï¼Œåˆ™ä½¿ç”¨å¤šç»´åº¦ç‰¹å¾åˆ¤æ–­
                if not is_hate:
                    features = TextProcessor.extract_hate_features(text, argument)  # ä½¿ç”¨ä¼ å…¥çš„text
                    is_hate = (
                        features['has_insult'] or
                        (features['has_negative_emoji'] and features['is_generalization']) or
                        features['has_dehumanizing']
                    )
                
                quad_list.append({
                    'target': target,
                    'argument': argument,
                    'group': group,
                    'is_hate': 1 if is_hate else 0
                })
        return quad_list
    
    @staticmethod
    def collate_fn(batch):
        # å¤„ç†å˜é•¿åºåˆ—
        input_ids = torch.stack([item['input_ids'] for item in batch])
        attention_mask = torch.stack([item['attention_mask'] for item in batch])
        
        # å¤„ç†å˜é•¿çš„groupså’Œis_hates
        groups = [item['groups'] for item in batch]
        is_hates = [item['is_hates'] for item in batch]
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'groups': groups,  # ä¿æŒä¸ºåˆ—è¡¨å½¢å¼
            'is_hates': is_hates  # ä¿æŒä¸ºåˆ—è¡¨å½¢å¼
        }

# æ–‡æœ¬å¤„ç†å·¥å…·ç±»
class TextProcessor:
    @staticmethod
    def extract_targets(text):
        # ä»…ä½¿ç”¨å…³é”®è¯åŒ¹é…
        targets = []
        words = jieba.lcut(text)
        for word in words:
            for group in TARGET_KEYWORDS.values():
                if word in group:
                    targets.append(word)
        return targets if targets else ['NULL']

    @staticmethod 
    def extract_arguments(text, targets):
        # ç®€å•è¿”å›æ•´ä¸ªæ–‡æœ¬ä½œä¸ºè®ºç‚¹
        return [text]

    @staticmethod
    def extract_hate_features(text, argument):
        features = {
            'has_hate_keyword': any(kw in argument for kw in HATE_KEYWORDS),
            'has_insult': len(re.findall(r'[å‚»ç¬¨è ¢è´±ä¸‘æ‡’è„å]', argument)) > 0,
            'has_negative_emoji': len(re.findall(r'[ğŸ¶ğŸ¤®ğŸ¤¢ğŸ¤¬ğŸ‘ğŸ˜“ğŸ”ª]', argument)) > 0,
            'is_generalization': any(word in argument for word in ['éƒ½', 'å…¨', 'æ€»æ˜¯']),
            'has_dehumanizing': any(term in argument for term in ['ä¸œè¥¿', 'è´§', 'ç©æ„å„¿'])
        }
        return features

def predict_quads(text, model, tokenizer, device='cuda'):
    processor = TextProcessor()
    targets = processor.extract_targets(text)
    arguments = processor.extract_arguments(text, targets)
    
    quads = []
    for target, arg in zip(targets, arguments):
        # åˆ¤æ–­group
        group = 'others'
        for g, kws in TARGET_KEYWORDS.items():
            if target in kws:
                group = g
                break
        
        # æƒ…æ„Ÿåˆ†æåˆ¤æ–­hate/non-hate
        sentiment = HanLP.sentiment_analysis(arg)
        is_hate = 'hate' if sentiment < 0 else 'non-hate'
        
        quads.append(f"{target} | {arg} | {group} | {is_hate}")
    
    return " [SEP] ".join(quads) + " [END]" if quads else " |  | non-hate | non-hate [END]"

# ä¿®æ”¹è®­ç»ƒå‡½æ•°ä»¥æ”¯æŒå››å…ƒç»„é¢„æµ‹
def finetune_chatglm():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ä¿®æ”¹æ¨¡å‹åŠ è½½æ–¹å¼ä¸ºINT4é‡åŒ–ç‰ˆæœ¬
    model_name = "/kaggle/input/bert/transformers/default/1/bert-base-chinese"
    tokenizer = BertTokenizer.from_pretrained(model_name)
    model = BertModel.from_pretrained(model_name).to(device)
    
    # æ·»åŠ æ•°æ®é›†åŠ è½½
    train_dataset = HateSpeechDataset(
        '/kaggle/input/hate-identification/train.json',  # è¯·ç¡®è®¤å®é™…è·¯å¾„
        tokenizer,
        max_length=128
    )
    train_loader = DataLoader(
        train_dataset,
        batch_size=32,
        shuffle=True,
        collate_fn=HateSpeechDataset.collate_fn  # ä½¿ç”¨è‡ªå®šä¹‰collateå‡½æ•°
    )
    
    # æ·»åŠ å››å…ƒç»„é¢„æµ‹å¤´
    model.group_classifier = torch.nn.Linear(model.config.hidden_size, 29).cuda()  # ä¿®æ”¹ä¸º29ä¸ªç±»åˆ«
    model.hate_classifier = torch.nn.Linear(model.config.hidden_size, 1).cuda()
    
    # ä¿®æ”¹ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡ï¼ŒBERTé€šå¸¸ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
    group_criterion = nn.CrossEntropyLoss()
    hate_criterion = nn.BCEWithLogitsLoss()
    
    # åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ æ¢¯åº¦ç´¯ç§¯
    gradient_accumulation_steps = 4
    
    # è®­ç»ƒå¾ªç¯
    epoch_num = 15
    
    torch.cuda.empty_cache()
    model.train()
    # ä¿®æ”¹è®­ç»ƒå¾ªç¯éƒ¨åˆ†
    for epoch in range(epoch_num):
        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epoch_num}')
        
        for batch_idx, batch in enumerate(progress_bar):
            optimizer.zero_grad()
            
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(input_ids, attention_mask=attention_mask)
            cls_embedding = outputs.last_hidden_state[:, 0, :]
            
            # è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„æŸå¤±
            total_loss = 0
            batch_group_loss = 0
            batch_hate_loss = 0
            
            # å¤„ç†æ¯ä¸ªæ ·æœ¬çš„å››å…ƒç»„
            for i in range(len(batch['groups'])):
                # è·å–å½“å‰æ ·æœ¬çš„é¢„æµ‹å’Œæ ‡ç­¾
                group_logits = model.group_classifier(cls_embedding[i].unsqueeze(0))
                hate_logits = model.hate_classifier(cls_embedding[i].unsqueeze(0))
                
                # ç¡®ä¿æ ‡ç­¾å½¢çŠ¶åŒ¹é…
                group_labels = batch['groups'][i].to(device).long()
                hate_labels = batch['is_hates'][i].to(device).float()
                
                # è®¡ç®—æŸå¤±
                group_loss = group_criterion(
                    group_logits.repeat(group_labels.size(0), 1),
                    group_labels
                )
                hate_loss = hate_criterion(
                    hate_logits.repeat(hate_labels.size(0), 1).squeeze(-1),
                    hate_labels
                )
                
                total_loss += (group_loss + hate_loss) / gradient_accumulation_steps
            
            total_loss.backward()
            
            if (batch_idx + 1) % gradient_accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
            
            progress_bar.set_postfix({'loss': total_loss.item()})
    
    # ä¿å­˜å®Œæ•´æ¨¡å‹
    print("æ¨¡å‹å·²ä¿å­˜ï¼")
    torch.save(model.state_dict(), '/kaggle/working/bert_model.pth')

if __name__ == "__main__":
    finetune_chatglm()