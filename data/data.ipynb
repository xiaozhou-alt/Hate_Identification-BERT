{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据量: 3200条\n",
      "测试集数据量: 800条\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 加载原始数据\n",
    "with open('data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 划分索引保持原始格式\n",
    "train_idx, test_idx = train_test_split(\n",
    "    range(len(data)),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=[1 if 'hate' in d['output'] else 0 for d in data]\n",
    ")\n",
    "\n",
    "# 按索引划分数据\n",
    "train_data = [data[i] for i in train_idx]\n",
    "test_data = [data[i] for i in test_idx]\n",
    "\n",
    "# 统计数据量\n",
    "print(f\"训练集数据量: {len(train_data)}条\")\n",
    "print(f\"测试集数据量: {len(test_data)}条\")\n",
    "\n",
    "# 保存划分后的数据（保持原始JSON格式）\n",
    "with open('train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open('test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def parse_quadruples(output_str):\n",
    "    \"\"\"解析output字段为四元组列表\"\"\"\n",
    "    quads = []\n",
    "    for quad in output_str.split('[SEP]'):\n",
    "        parts = [p.strip() for p in quad.split('|')]\n",
    "        if len(parts) >= 4:\n",
    "            quads.append({\n",
    "                'target': parts[0],\n",
    "                'argument': parts[1],\n",
    "                'target_group': parts[2],\n",
    "                'hateful': parts[3].replace('[END]', '').strip()\n",
    "            })\n",
    "    return quads\n",
    "\n",
    "def analyze_data(data):\n",
    "    stats = {\n",
    "        'total_samples': len(data),\n",
    "        'hate_quads': 0,\n",
    "        'non_hate_quads': 0,\n",
    "        'avg_quads_per_sample': 0\n",
    "    }\n",
    "    \n",
    "    target_groups = []\n",
    "    hate_status = []\n",
    "    \n",
    "    for sample in data:\n",
    "        quads = parse_quadruples(sample['output'])\n",
    "        stats['avg_quads_per_sample'] += len(quads)\n",
    "        for quad in quads:\n",
    "            target_groups.append(quad['target_group'])\n",
    "            if quad['hateful'] == 'hate':\n",
    "                stats['hate_quads'] += 1\n",
    "            else:\n",
    "                stats['non_hate_quads'] += 1\n",
    "    \n",
    "    stats['avg_quads_per_sample'] /= stats['total_samples']\n",
    "    stats['target_group_dist'] = dict(Counter(target_groups))\n",
    "    \n",
    "    return stats\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('train.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    stats = analyze_data(data)\n",
    "    \n",
    "    # 保存基础统计\n",
    "    pd.DataFrame.from_dict(stats, orient='index').to_excel('output/basic_stats.xlsx')\n",
    "    \n",
    "    # 保存目标群体分布\n",
    "    pd.DataFrame.from_dict(\n",
    "        stats['target_group_dist'], \n",
    "        orient='index', \n",
    "        columns=['count']\n",
    "    ).to_excel('output/target_group_dist.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词云图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "仇恨言论词云已生成！\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_stopwords(filepath):\n",
    "    \"\"\"加载屏蔽词文件\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "def extract_argument(output_str):\n",
    "    \"\"\"提取两个|之间的Argument内容\"\"\"\n",
    "    parts = output_str.split(' | ')\n",
    "    if len(parts) >= 2:\n",
    "        return parts[1]  # 返回第二个部分（Argument）\n",
    "    return \"\"\n",
    "\n",
    "def generate_wordcloud(texts, save_path, stopwords=None):\n",
    "    \"\"\"生成词云，支持屏蔽词过滤\"\"\"\n",
    "    wordcloud = WordCloud(\n",
    "        font_path='simhei.ttf',\n",
    "        width=800,\n",
    "        height=600,\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_font_size=120\n",
    "    ).generate(' '.join(texts))\n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def analyze_hate_speech():\n",
    "    # 1. 加载数据\n",
    "    with open('train.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 2. 提取仇恨言论的Argument部分\n",
    "    hate_arguments = [extract_argument(d['output']) for d in data if 'hate' in d['output']]\n",
    "    \n",
    "    # 3. 加载屏蔽词\n",
    "    stopwords = load_stopwords('stopwords.txt')\n",
    "    \n",
    "    # 4. 生成词云\n",
    "    generate_wordcloud(hate_arguments, 'output/wordcloud.png', stopwords)\n",
    "    print(\"仇恨言论词云已生成！\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analyze_hate_speech()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取类别目标词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类别和类别之间可能包含的词汇会有相同的词汇，这种情况的话，在全部写入后，帮我遍历一遍，然后去除重复的，去除规则是这样的：\n",
    "\n",
    "- 1.当两个类别中的分类数不一样，保留分类数多的，如“Region”和“Region,others”保留后者中的那一条内容\n",
    "\n",
    "- 2.当两个类别中的分类数一样时，保留长度小的那个，如“Region”和“Sexism”含有的元素数量分别是20,13，那么保留后者中的那一条内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关键词已保存到 target_keywords.txt 文件中。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# 需要排除的词语列表\n",
    "exclude_words = [\n",
    "    '人','群体','我们','你们','这些人','某些人','部分人','广告','弹幕',\n",
    "    '哥们','西方紫苯','群体','组织','机构','政府','社会','国家','民族', \n",
    "    '种族','这个','那个','这','那','他们','他','她','她们','它们',\n",
    "    '它','那些人','有些人','有人','这个吧','那个吧','这吧','那吧','他们吧',\n",
    "    '他吧','她吧','她们吧','它们吧','它吧','这些人吧','某些人吧','部分人吧',\n",
    "    ,'相当一部分人'\n",
    "]\n",
    "\n",
    "# 加载训练数据\n",
    "with open('./train.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 初始化字典\n",
    "TARGET_KEYWORDS = defaultdict(list)\n",
    "\n",
    "# 遍历数据统计关键词部分\n",
    "for item in data:\n",
    "    for quad in item['output'].split(' [SEP] '):\n",
    "        parts = quad.split(' | ')\n",
    "        if len(parts) >= 4:\n",
    "            target = parts[0].strip()\n",
    "            group = parts[2].strip()\n",
    "            if target and group and target not in exclude_words:\n",
    "                TARGET_KEYWORDS[group].append(target)\n",
    "\n",
    "# 去重并输出到文件\n",
    "def remove_duplicates(keywords_dict):\n",
    "    # 创建一个反向映射：词汇 -> 类别列表\n",
    "    word_to_categories = defaultdict(list)\n",
    "    for category, words in keywords_dict.items():\n",
    "        for word in words:\n",
    "            word_to_categories[word].append(category)\n",
    "    \n",
    "    # 处理重复词汇\n",
    "    for word, categories in word_to_categories.items():\n",
    "        if len(categories) > 1:\n",
    "            # 按规则排序：1. 类别数量多的优先 2. 类别数量相同时，保留元素数量少的\n",
    "            categories.sort(\n",
    "                key=lambda c: (-len(c.split(',')), len(keywords_dict[c]))\n",
    "            )\n",
    "            # 保留第一个类别，从其他类别中删除该词汇\n",
    "            for category in categories[1:]:\n",
    "                if word in keywords_dict[category]:\n",
    "                    keywords_dict[category].remove(word)\n",
    "    \n",
    "    return keywords_dict\n",
    "\n",
    "# 去重处理\n",
    "TARGET_KEYWORDS = remove_duplicates(TARGET_KEYWORDS)\n",
    "\n",
    "# 输出到文件\n",
    "with open('h:\\\\project\\\\Hate_Identification\\\\data\\\\target_keywords.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('TARGET_KEYWORDS = {\\n')\n",
    "    for i, (k, v) in enumerate(TARGET_KEYWORDS.items()):\n",
    "        f.write(f'    \"{k}\": {v}')\n",
    "        if i < len(TARGET_KEYWORDS) - 1:\n",
    "            f.write(',\\n\\n')\n",
    "        else:\n",
    "            f.write('\\n\\n')\n",
    "    f.write('}\\n')\n",
    "print(\"关键词已保存到 target_keywords.txt 文件中\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
